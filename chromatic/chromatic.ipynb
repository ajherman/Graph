{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some new ideas...\n",
    "\n",
    "Let's start with independent sets. An independent set is a binary vector $x$, such that $x^TAx=0$, where $A$ is the adjacency matrix. Finding the independence number means maximizing the weight of $x$. Can we phrase this as an optimization problem? Trying to minimize $x^TAx$ is not a problem, since $A$ is a psd matrix. However, without additional constraints, this would just lead to $x=0$, which is clearly not optimal. \n",
    "\n",
    "What if we fix the size of the independent set, $K$? Then we are looking for vectors that have a fixed norm (either L1 or L2). Let's further restrict all of our values to lie between 0 and 1. If we fix $|x|$, then forcing $x$ to be binary corresponds to minimizing $|x|_1$ (I think). \n",
    "\n",
    "Proof: Suppose $|x|=\\sqrt{K}$. Note that since $x\\in[0,1]^N$, we have $1-x\\ge 0$, and so $x\\cdot(1-x)\\ge 0$. Therfore, $|x|_1=x\\cdot 1\\ge x\\cdot x=|x|^2=K$. Further, equality holds exactly when $x$ is binary, in which case $|x|_1=K$. \n",
    "\n",
    "Therefore, if there is an independent set of size $K$, we should be able to find it be minimizing $x^TAx+|x|_1$ with the constraint $|x|=\\sqrt{K}$, $x\\in[0,1]^N$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "class energyModel(torch.nn.Module):\n",
    "    def __init__(self, A, lam=1.0):\n",
    "        super(energyModel, self).__init__()\n",
    "        self.A = A\n",
    "        self.lam = lam\n",
    "\n",
    "    def forward(self, x):\n",
    "        energy = x.t()@self.A@x+self.lam*torch.norm(x,1)\n",
    "        return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1:  25.566421508789062\n",
      "Independence:  74.85324096679688\n",
      "\n",
      "L1:  11.855396270751953\n",
      "Independence:  8.449209213256836\n",
      "\n",
      "L1:  6.857504844665527\n",
      "Independence:  0.08543948829174042\n",
      "\n",
      "L1:  5.118616580963135\n",
      "Independence:  0.0\n",
      "\n",
      "L1:  4.3925251960754395\n",
      "Independence:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.insert(1, '../OldStuff')\n",
    "# from GraphFun import *\n",
    "\n",
    "K = torch.tensor(4.0) # Size of independent set\n",
    "\n",
    "# Petersen graph adjacency array\n",
    "A = np.array([\n",
    "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
    "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "       [0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "       [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "       [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "       [1, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
    "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "# G = genJohnsonGraph(4,2,1)\n",
    "# A = getAdjArray(G)\n",
    "# print(A)\n",
    "\n",
    "# A=np.array([[0, 1, 1, 1, 1, 0], # Octahedron\n",
    "#        [1, 0, 1, 1, 0, 1],\n",
    "#        [1, 1, 0, 0, 1, 1],\n",
    "#        [1, 1, 0, 0, 1, 1],\n",
    "#        [1, 0, 1, 1, 0, 1],\n",
    "#        [0, 1, 1, 1, 1, 0]])\n",
    "\n",
    "# A = np.load(\"Jvki_graphs/J_10_4.npy\")\n",
    "# A = torch.tensor(A,dtype=torch.float32)\n",
    "# N = A.shape[0] # Number of vertices\n",
    "\n",
    "# Init x\n",
    "x = torch.randn(N,requires_grad=True)\n",
    "x.data = torch.abs(x.data)\n",
    "x.data = torch.clamp(x.data,0,1) # Project x to [0,1]\n",
    "x.data = x.data*torch.sqrt(K)/torch.norm(x.data,2) # Normalize x\n",
    "\n",
    "# Create model / optimizer\n",
    "model = energyModel(A)\n",
    "optimizer = torch.optim.Adam([x], lr=0.001)\n",
    "\n",
    "# Minimize energy of x\n",
    "for i in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    energy = model(x)\n",
    "    energy.backward()\n",
    "    optimizer.step()\n",
    "    x.data = torch.clamp(x.data,0,1) # Project x to [0,1]\n",
    "    x.data = x.data*torch.sqrt(K)/torch.norm(x.data,2) # Normalize x\n",
    "    x.data = torch.clamp(x.data,0,1)\n",
    "    if i%1000==0:\n",
    "        print(\"L1: \",torch.norm(x,1).item())\n",
    "        print(\"Independence: \",(x.t()@A@x).item())\n",
    "        # print(\"Energy: \",energy)\n",
    "        print(\"\")\n",
    "        # print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like this isn't really work. At least it's not working well. Even if we set $\\lambda=0$, it can still fail to find the optimum. I'm guessing this is being the L2 norm constraint makes the optimization non-convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another idea I want to explore is using symmetric polynomials to try to find colorings. Suppose we thinking of coloring Johnson graphs as a problem of mapping $k$-sets of integers (modulo v), to elements of $v$. The goal is to find a symmetric function in the variable $x_1,\\ldots, x_k$ such that 1) $f(\\bar{x})$ is injective in each coordinate, given fixed values of the other coordinates and 2) the range of $f$ is as small as possible.\n",
    "\n",
    "The first condition is necessary so that whenever to vertices differ in a single element, $f$ will assign them different values. Note further, that (I think) every symmetric function on $[v]$ is a symmetric polynomial...maybe. \n",
    "\n",
    "That's one approach. A more general approach would be to do some sort of annealing. Let's say we are trying to $K$-color our graph. We assign a $K$-vector to each vertex. Our goal is to end up with one-hot vectors that are orthogonal for adjacenct vertices. But to start, we will allow the vectors to take on a continuum of values. The neighbors of a vector will be used to flip it's value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best:  0.0\n",
      "Mean:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Get graph\n",
    "v,k = 5,2\n",
    "A = np.load(\"Jvki_graphs/J_\"+str(v)+\"_\"+str(k)+\".npy\")\n",
    "A = torch.tensor(A,dtype=torch.float32)\n",
    "N = A.shape[0] # Number of vertices\n",
    "\n",
    "c = 5\n",
    "beta=0.02 # Temperature\n",
    "# x = torch.randn((N,c),requires_grad=True) # Logits\n",
    "\n",
    "# p = torch.softmax(0.1*x,dim=1) # probabilities\n",
    "# loss = torch.sum(A*(p@p.t())) # Covariance matrix\n",
    "\n",
    "# optimizer = torch.optim.Adam([x], lr=0.1)\n",
    "\n",
    "# Minimize loss\n",
    "# best_val = v\n",
    "vals = []\n",
    "for trial in range(50):\n",
    "    x = torch.randn((N,c),requires_grad=True) # Logits\n",
    "    optimizer = torch.optim.Adam([x], lr=5.0)\n",
    "    for i in range(2000):\n",
    "        optimizer.zero_grad()\n",
    "        p = torch.softmax(beta*x,dim=1) # probabilities\n",
    "        # loss = torch.sum(torch.tensor([p[:,i].t()@A@p[:,i] for i in range(c)])) # Entropy\n",
    "        loss = torch.sum(A*(p@p.t())) # Covariance matrix\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print(\"Loss: \",loss.item())\n",
    "    # print(\"Independence: \",torch.sum(A*(p@p.t())).item())\n",
    "    # print(\"\")\n",
    "\n",
    "    coloring = torch.softmax(20*x,dim=1)\n",
    "    # print(p)\n",
    "    # print(p>0.5)\n",
    "    val = torch.sum(A*(coloring@coloring.t())).item()\n",
    "    vals.append(val)\n",
    "    if val<0.1:\n",
    "        break\n",
    "    # best_val = min(best_val,val)\n",
    "vals = np.array(vals)\n",
    "print(\"Best: \",np.min(vals))\n",
    "print(\"Mean: \",np.mean(vals<0.1))\n",
    "# print(\"Std: \",np.std(vals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, our underlying variable, $x$, can be an arbitrary vector. And we take the probability to be its softmax. Then we trying to get adjecent vertices to have orthgonal probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Petersen graph\n",
    "A = np.array([\n",
    "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
    "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "       [0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "       [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "       [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "       [1, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
    "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "A = torch.tensor(A,dtype=torch.float32) #-torch.eye(10)\n",
    "N = A.shape[0] # Number of vertices\n",
    "n_steps = 800\n",
    "c = 3\n",
    "beta=0.02 # Temperature\n",
    "\n",
    "# Minimize loss\n",
    "vals = []\n",
    "x = torch.randn((N,c),requires_grad=True) # Logits\n",
    "x.data = 10*x.data\n",
    "optimizer = torch.optim.Adam([x], lr=0.1)\n",
    "points =[]\n",
    "losses = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad()\n",
    "    p = torch.softmax(beta*x,dim=1) # probabilities\n",
    "    points.append(p.detach())\n",
    "    loss = torch.sum(A*(p@p.t())) # Covariance matrix\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# print(\"Loss: \",loss.item())\n",
    "# print(\"Independence: \",torch.sum(A*(p@p.t())).item())\n",
    "# print(\"\")\n",
    "\n",
    "coloring = torch.softmax(20*x,dim=1)\n",
    "# print(p)\n",
    "# print(p>0.5)\n",
    "val = torch.sum(A*(coloring@coloring.t())).item()\n",
    "print(val)\n",
    "points = torch.stack(points).numpy()\n",
    "R = np.array([[1/np.sqrt(3),1/np.sqrt(3),1/np.sqrt(3)],\n",
    "              [1/np.sqrt(2),-1/np.sqrt(2),0],\n",
    "              [1/np.sqrt(6),1/np.sqrt(6),-2/np.sqrt(6)]])\n",
    "\n",
    "in_plane = (points@R.T)[:,:,1:] #-np.array([1]+[0]*(c-1))/np.sqrt(c)\n",
    "\n",
    "# Animate points of in_plane as moving scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "scat = ax.scatter(in_plane[0,:,0], in_plane[0,:,1],s=30)\n",
    "\n",
    "ax.set_xlim([-0.8,0.8])\n",
    "ax.set_ylim([-0.8,0.8])\n",
    "\n",
    "def update(frame_number):\n",
    "    scat.set_offsets(in_plane[frame_number])\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=n_steps, interval=50)\n",
    "ani.save('test.gif', writer='imagemagick', fps=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another version. Rather than using softmax, we just normalize the vectors at each step while trying to push the angle between adjacent vertices towards 90. The probability will be defined by squaring all the entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028301675338298082\n"
     ]
    }
   ],
   "source": [
    "# Petersen graph\n",
    "A = np.array([\n",
    "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
    "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "       [0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "       [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "       [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "       [1, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
    "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "A = torch.tensor(A,dtype=torch.float32) #-torch.eye(10)\n",
    "N = A.shape[0] # Number of vertices\n",
    "n_steps = 800\n",
    "c = 3\n",
    "beta=0.02 # Temperature\n",
    "\n",
    "# Minimize loss\n",
    "vals = []\n",
    "\n",
    "# Init x\n",
    "x = torch.randn((N,c),requires_grad=True) # Logits\n",
    "x.data = torch.nn.functional.normalize(x.data, p=2, dim=1)\n",
    "\n",
    "optimizer = torch.optim.Adam([x], lr=0.1)\n",
    "# optimizer = torch.optim.SGD([x], lr=0.005)\n",
    "points =[]\n",
    "losses = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y = x**2\n",
    "    # p = y/torch.sum(y,dim=1,keepdim=True) # probabilities\n",
    "    total = torch.sum(y,dim=1,keepdim=True)\n",
    "    # print(total)\n",
    "    p = y#/total\n",
    "    # p = torch.nn.functional.normalize(y, p=1, dim=1)\n",
    "\n",
    "    points.append(p.detach())\n",
    "    loss = torch.sum(A*(p@p.t())) # Covariance matrix\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    x.data = torch.nn.functional.normalize(x.data, p=2, dim=1)\n",
    "\n",
    "# print(\"Loss: \",loss.item())\n",
    "# print(\"Independence: \",torch.sum(A*(p@p.t())).item())\n",
    "# print(\"\")\n",
    "\n",
    "# coloring = x**2 #torch.softmax(20*x,dim=1)\n",
    "coloring = torch.softmax(10*p,dim=1)\n",
    "\n",
    "# print(p)\n",
    "# print(p>0.5)\n",
    "val = torch.sum(A*(coloring@coloring.t())).item()\n",
    "print(val)\n",
    "\n",
    "points = torch.stack(points).numpy()\n",
    "R = np.array([[1/np.sqrt(3),1/np.sqrt(3),1/np.sqrt(3)],\n",
    "              [1/np.sqrt(2),-1/np.sqrt(2),0],\n",
    "              [1/np.sqrt(6),1/np.sqrt(6),-2/np.sqrt(6)]])\n",
    "\n",
    "in_plane = (points@R.T)[:,:,1:] #-np.array([1]+[0]*(c-1))/np.sqrt(c)\n",
    "\n",
    "# Animate points of in_plane as moving scatter plot\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scat = ax.scatter(in_plane[0,:,0], in_plane[0,:,1],s=30)\n",
    "\n",
    "ax.set_xlim([-0.8,0.8])\n",
    "ax.set_ylim([-0.9,0.6])\n",
    "\n",
    "def update(frame_number):\n",
    "    scat.set_offsets(in_plane[frame_number])\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=n_steps, interval=50)\n",
    "ani.save('test.gif', writer='imagemagick', fps=30)\n",
    "plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
